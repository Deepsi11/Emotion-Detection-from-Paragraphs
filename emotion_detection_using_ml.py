# -*- coding: utf-8 -*-
"""Emotion_Detection_Using_ML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1H1FWG6yPwugVHH1pAzc-_tjONX8cynpr

#**Automate Emotion Detection For Paragraphs Using Machine Learning**

##**Importing Libraries**
"""

#Import Pandas and NumPy libraries
import pandas as pd
import numpy as np

# Import NLTK and Gensim modules
import nltk
from nltk.corpus import stopwords

# Importing FastText model from Gensim
from gensim.models import FastText

# Import tqdm and setup notebook integration
from tqdm._tqdm_notebook import tqdm_notebook
tqdm_notebook.pandas()

# Importing additional components from NLTK
# Importing WordNetLemmatizer for word lemmatization
from nltk.stem import WordNetLemmatizer

# Import contraction_map and other utilities
# Assuming this is a custom module for handling contractions
from contractions import contraction_map

# Importing string module for string operations
import string

 # Importing re module for regular expressions
import re

# Initialize NLTK resources if necessary
nltk.download('stopwords')
nltk.download('wordnet')

# Initialize stopwords and lemmatizer
# Set of English stopwords
stop_words = set(stopwords.words('english'))

# WordNetLemmatizer instance for word lemmatization
lemmatizer = WordNetLemmatizer()

"""##**Data cleaning**"""

# Loading the dataset
data = pd.read_csv('/content/text_emotion (1).csv')

# Having a look at the dataset
data.head()

# Getting count of the emotions present in the dataset
data.sentiment.value_counts()

# Creating a copy of the dataset
df = data.copy()

# Checking the data types present
df.dtypes

# Dropping rows with other emotion labels, i.e rounding number of emotions to only 5 basic ones
df = df.drop(df[df.sentiment == 'boredom'].index)
df = df.drop(df[df.sentiment == 'enthusiasm'].index)
df = df.drop(df[df.sentiment == 'empty'].index)
df = df.drop(df[df.sentiment == 'fun'].index)
df = df.drop(df[df.sentiment == 'relief'].index)
df = df.drop(df[df.sentiment == 'surprise'].index)
df = df.drop(df[df.sentiment == 'love'].index)
df = df.drop(df[df.sentiment == 'hate'].index)

# Having a count of each sentiment as reduced to 5 emotions.
df.sentiment.value_counts()

# Lets have a look at our dataset
df

# Dropping the "author" column from the dataset
df = df.drop('author',axis=1)
df

# Resetting the index
df.reset_index(drop=True,inplace=True)

# having a look at our Data
df

# Checking the number of columns and rows present in our dataset
df.shape

# Dropping the "tweet_id" column
df.drop('tweet_id',axis=1,inplace=True)

# Keeping two columns in our dataset such as 'Sentiment' and 'text'
df.columns = ['sentiment','text']

"""#**Data pre-processing**

##**Stopword Exclusion**
"""

#Using Sentiment lexicons to be excluded from stopwords
df_pos = pd.read_csv('./lexicons/positive.csv')
df_neg = pd.read_csv('./lexicons/negative.csv')

"""##**Some Required functions for Text Cleaning**

##**Function to format the text**
"""

# Function to format the text
    def expand_text(text):

      # Convert text to lowercase
      text = text.lower()

      # Replace backticks (`) with single quotes (')
      text = text.replace("`", "'")

      # Expand Contractions
      # Assuming contraction_map is a dictionary of contractions to expansions
      contraction_dict = contraction_map

      # Get list of contraction keys
      contraction_keys = list(contraction_dict.keys())

      # Iterate through each word in the text
      for word in text.split():

          # Check if the word is a contraction (in contraction_keys)
          if word in contraction_keys:

              # Replace the contraction with its expanded form from contraction_dict
              text = text.replace(word, contraction_dict[word])

          else:

            # If word is not a contraction, continue to next word
            continue

      return text

"""##**Function for text cleaning**"""

# Function for text cleaning
def clean_text(text):

    # Remove punctuation using translation with None to remove all punctuation
    text = text.translate(string.punctuation)

    # Convert text to lowercase and split into words
    text = text.lower().split()

    # Assuming df_pos and df_neg are DataFrames or objects with 'words' attribute
    df_pos_words = list(df_pos.words)
    df_neg_words = list(df_neg.words)

    # Create lists of positive and negative words with spaces removed
    positive = []
    for i in range(0,len(df_pos_words)):
        positive.append(df_pos_words[i].lower().replace(" ",""))

    negative = []
    for i in range(0,len(df_neg_words)):
        negative.append(df_neg_words[i].lower().replace(" ",""))

    # Convert positive and negative lists to sets for efficient membership checking
    pos_set = set(positive)
    neg_set = set(negative)

    # Define keywords set including standard stopwords and additional keywords
    keywords = set(["above","and","below","not"])
    keywords.update(pos_set)
    keywords.update(neg_set)

    # Retrieve NLTK stopwords and remove keywords to create stopword set
    stopwords_set = set(stopwords.words('english'))
    stops = stopwords_set - keywords

    # Remove stopwords from text
    text = [w for w in text if not w in stops]
    text = " ".join(text)

    # Perform additional text cleaning using regular expressions
    text = re.sub(r"[^A-Za-z0-9^,!./\'+-=]"," ",text)
    text = re.sub(r"what's","what is",text)
    text = re.sub(r"\'s"," ",text)
    text = re.sub(r"\'ve"," have ",text)
    text = re.sub(r"n't"," not ",text)
    text = re.sub(r"i'm"," i am ",text)
    text = re.sub(r"\'re"," are ",text)
    text = re.sub(r"\'d", " would ",text)
    text = re.sub(r"\'ll", " will ",text)
    text = re.sub(r","," ",text)
    text = re.sub(r"\."," ",text)
    text = re.sub(r"!"," ! ",text)
    text = re.sub(r"\/"," ",text)
    text = re.sub(r"\^"," ^ ",text)
    text = re.sub(r"\+"," + ",text)
    text = re.sub(r"\-"," - ",text)
    text = re.sub(r"\="," = ",text)
    text = re.sub(r"'"," ",text)
    text = re.sub(r"(\d+)(k)",r"\g<1>000",text)
    text = re.sub(r":", " : ",text)
    text = re.sub(r" e g "," eg ",text)
    text = re.sub(r"b g "," bg ",text)
    text = re.sub(r" u s "," american ",text)
    text = re.sub(r"\0s","0",text)
    text = re.sub(r"e - mail","email",text)
    text = re.sub(r"\s{2,}"," ",text)

    # Split text into words again
    text = text.split()

    # Initialize WordNetLemmatizer for lemmatization
    lemmatizer = WordNetLemmatizer()

    # Lemmatize words
    lemmatized_words = [lemmatizer.lemmatize(word) for word in text]

    # Join lemmatized words into cleaned text
    text = " ".join(lemmatized_words)

    return text

# Let's have a look at our data again
df

# Apply the expand_text function to each element in the 'text' column of DataFrame 'df'
df['text'] = df['text'].progress_apply(lambda x : expand_text(x))

# Downloading stopwords and wordnet
nltk.download('stopwords')
nltk.download('wordnet')

# Apply the clean_text function to each element in the 'text' column of DataFrame 'df'
df['text'] = df['text'].progress_apply(lambda x: clean_text(x))

# Counting the value of each sentiment
df.sentiment.value_counts()

# Let's see how our dataset look like
df

"""##**Label Encoding**"""

#Feature encoding
from sklearn.preprocessing import LabelEncoder
lbl_enc = LabelEncoder()
y = lbl_enc.fit_transform(df.sentiment.values)

#For building a dataframe for mapping emotions to label number
y_series = pd.Series(y)

#inverse transform to find  mapped emotions
emo_cols_series = pd.Series(lbl_enc.inverse_transform(y))

# Creating dataframe
emo_df = pd.DataFrame()

# Create a new DataFrame emo_df by concatenating emo_cols_series and y_series along axis 1 (columns)
emo_df = pd.concat([emo_cols_series,y_series],axis=1)

# Rename columns of emo_df to 'emotion' and 'Label_mapped'
emo_df.columns=['emotion','Label_mapped']

# Drop duplicate rows from emo_df based on all columns
emo_df = emo_df.drop_duplicates()

# Sort emo_df by the 'Label_mapped' column in ascending order
emo_df.sort_values(by=['Label_mapped'], ascending=True)

"""##**Importing Required Libraries**"""

#Train test split
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.naive_bayes import MultinomialNB

"""##**Train-Test Split**"""

# Splitting the dataset into train and test
X_train,X_val,y_train,y_val = train_test_split(df.text.values,df.sentiment.values,random_state=42, test_size=0.1, shuffle=True)

"""##**Using Tf-idf**"""

#Extracting Tf-idf features
from sklearn.feature_extraction.text import TfidfVectorizer
tfidf = TfidfVectorizer(min_df=3,  max_features=None,
            strip_accents='unicode', analyzer='word',token_pattern=r'\w{1,}',
            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1)

#TF-idf approach
tfidf.fit(list(X_train) + list(X_val))
X_train_tfidf = tfidf.transform(X_train)
X_val_tfidf = tfidf.transform(X_val)

"""##**Using Countvectorizer**"""

#Count Vector approach
from sklearn.feature_extraction.text import CountVectorizer
count_vec = CountVectorizer(analyzer='word',token_pattern=r'\w{1,}',
            ngram_range=(1, 3))
count_vec.fit(list(X_train) + list(X_val))

# Implementing countvectorizer in train and validation set
X_train_count_vec = count_vec.transform(X_train)
X_val_count_vec = count_vec.transform(X_val)

"""#**Final Result**

##**Using tf-idf for models**

#**Model building**

#**Multinomial Naive Bayes Model**
"""

# Using Multinomial naive bayes model
from sklearn.metrics import accuracy_score
from sklearn.naive_bayes import MultinomialNB
nb = MultinomialNB()
nb.fit(X_train_tfidf, y_train)
y_pred = nb.predict(X_val_tfidf)
print('naive bayes tfidf accuracy %s' % accuracy_score(y_pred, y_val))

"""#**Random Forest Classifier Model**"""

# Using Random Forest Classifier Model
from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators=500)
rf.fit(X_train_tfidf,y_train)
y_pred2 = rf.predict(X_val_tfidf)
print('Random forest tfidf accuracy %s' % accuracy_score(y_pred2, y_val))

"""#**SGD Classifier Model**"""

# Using SGD Classifier Model
from sklearn.linear_model import SGDClassifier
lsvm = SGDClassifier(alpha=0.001, random_state=5, max_iter=15, tol=None)
lsvm.fit(X_train_tfidf, y_train)
y_pred_sgd = lsvm.predict(X_val_tfidf)
print('SGD using tfidf accuracy %s' % accuracy_score(y_pred_sgd, y_val))

"""#**Logistic Regression Model**"""

# Using Logistic Regression Model
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression(C=1.0)
lr.fit(X_train_tfidf,y_train)
y_pred = lr.predict(X_val_tfidf)
print('Accuracy with Logistic Regression and Tf-idf is {}'.format(accuracy_score(y_pred,y_val)))

"""#**USING COUNTVECTORIZER FOR ALL THE MODELS**

##**Multinomial Naive Bayes**
"""

#Using Countvec for model
from sklearn.metrics import accuracy_score
from sklearn.naive_bayes import MultinomialNB
nb = MultinomialNB()
nb.fit(X_train_count_vec, y_train)
y_pred = nb.predict(X_val_count_vec)
print('naive bayes count_vec accuracy %s' % accuracy_score(y_pred, y_val))

"""##**SGD Classifier**"""

# Using SGD Classifier
from sklearn.linear_model import SGDClassifier
lsvm = SGDClassifier()
lsvm.fit(X_train_count_vec, y_train)
y_pred = lsvm.predict(X_val_count_vec)
print('svm using countvec accuracy %s' % accuracy_score(y_pred, y_val))

"""#**Logistic Regression**"""

# Implementing countvectorizer on Logistic Regression
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression(C=1.0)
lr.fit(X_train_count_vec,y_train)
y_pred = lr.predict(X_val_count_vec)
print('Accuracy with Logistic Regression and CountVec is {}'.format(accuracy_score(y_pred,y_val)))

"""##**Generate classification report and confusion matrix for SGD + TF-IDF model**

##**Classification Report**
"""

# Importing the classification Report
from sklearn.metrics import classification_report

"""##**Precision, Recall, Accuracy and F1-Score**"""

# Printing the overall accuracy using Precision, Recall, accuracy and F1-score.
print(classification_report(y_val,y_pred_sgd))